{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FQELiSJnPtOD",
        "outputId": "f4c0dc56-19c5-4ad7-f5f1-c498526265f0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of Classes: 12\n",
            "Initial Feature Count: 46\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/feature_selection/_univariate_selection.py:783: UserWarning: k=60 is greater than n_features=46. All the features will be returned.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Selected Features Count: 46\n",
            "Selected Features: ['Number', 'Weight', 'IAT', 'Rate', 'Srate', 'Covariance', 'Duration', 'flow_duration', 'Tot size', 'Max', 'Tot sum', 'urg_count', 'fin_count', 'AVG', 'Min', 'Header_Length', 'Magnitue', 'rst_count', 'syn_count', 'Protocol Type', 'Variance', 'TCP', 'ack_count', 'syn_flag_number', 'UDP', 'ack_flag_number', 'ICMP', 'rst_flag_number', 'psh_flag_number', 'fin_flag_number', 'Radius', 'DNS', 'Drate', 'HTTP', 'Telnet', 'HTTPS', 'Std', 'IRC', 'ARP', 'SSH', 'DHCP', 'SMTP', 'cwr_flag_number', 'ece_flag_number', 'IPv', 'LLC']\n",
            "\n",
            "Evaluation Metrics:\n",
            "Accuracy: 0.9502162859527965\n",
            "Precision: 0.9502137242954187\n",
            "Recall: 0.9502162859527965\n",
            "F1 Score: 0.9498787553933904\n",
            "\n",
            "Classification Report:\n",
            "                         precision    recall  f1-score   support\n",
            "\n",
            "          BenignTraffic       1.00      1.00      1.00      1748\n",
            "        DDoS-ICMP_Flood       1.00      1.00      1.00     11003\n",
            "      DDoS-PSHACK_Flood       1.00      1.00      1.00      6237\n",
            "       DDoS-RSTFINFlood       1.00      1.00      1.00      6318\n",
            "         DDoS-SYN_Flood       0.90      0.96      0.93      6195\n",
            "DDoS-SynonymousIP_Flood       0.96      0.94      0.95      5453\n",
            "         DDoS-TCP_Flood       0.93      0.96      0.95      6875\n",
            "         DDoS-UDP_Flood       0.91      0.94      0.92      8430\n",
            "          DoS-SYN_Flood       0.89      0.81      0.85      3047\n",
            "          DoS-TCP_Flood       0.93      0.88      0.90      4012\n",
            "          DoS-UDP_Flood       0.89      0.84      0.87      5059\n",
            "     Mirai-greeth_flood       1.00      0.99      1.00      1508\n",
            "\n",
            "               accuracy                           0.95     65885\n",
            "              macro avg       0.95      0.94      0.95     65885\n",
            "           weighted avg       0.95      0.95      0.95     65885\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            "[[ 1747     0     0     0     0     0     0     0     0     0     1     0]\n",
            " [    0 10995     1     1     2     1     0     0     1     0     1     1]\n",
            " [    1     0  6232     0     0     0     1     1     1     0     1     0]\n",
            " [    0     0     0  6314     0     0     0     1     0     0     3     0]\n",
            " [    0     1     1     1  5923    86     1     0   181     0     0     1]\n",
            " [    0     2     1     0   196  5125     0     1   128     0     0     0]\n",
            " [    0     4     0     1     1     0  6619     1     0   248     1     0]\n",
            " [    0     4     4     0     3     1     2  7902     2     0   512     0]\n",
            " [    1     4     4     0   460   101     1     1  2472     2     0     1]\n",
            " [    1     1     3     0     1     0   486     1     2  3516     1     0]\n",
            " [    0     1     0     0     0     0     2   791     1     1  4262     1]\n",
            " [    4     4     0     0     0     0     0     0     1     0     1  1498]]\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_selection import SelectKBest, chi2, RFE\n",
        "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix, precision_score, recall_score, f1_score\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Step 1: Load dataset\n",
        "df = pd.read_csv('data.csv')\n",
        "\n",
        "# Step 2: Keep only the 12 most common classes\n",
        "top_12_classes = df['label'].value_counts().index[:12]\n",
        "df_12 = df[df['label'].isin(top_12_classes)]\n",
        "\n",
        "# Step 3: Separate features and labels\n",
        "X = df_12.drop(\"label\", axis=1, errors='ignore')\n",
        "y = df_12[\"label\"]\n",
        "\n",
        "# Step 4: Preprocessing (encode categoricals + fill missing values + scale for chi2)\n",
        "X = pd.get_dummies(X, drop_first=True)\n",
        "X.fillna(X.mean(), inplace=True)\n",
        "X = pd.DataFrame(MinMaxScaler().fit_transform(X), columns=X.columns)  # chi2 requires non-negative values\n",
        "\n",
        "print(\"Number of Classes:\", y.nunique())\n",
        "print(\"Initial Feature Count:\", X.shape[1])\n",
        "\n",
        "# Step 5: Hybrid Feature Selection to get 46 features\n",
        "# Filter Method - Chi-Square\n",
        "filter_selector = SelectKBest(score_func=chi2, k=60)  # First filter to top 60 features\n",
        "X_filtered = filter_selector.fit_transform(X, y)\n",
        "filter_scores = filter_selector.scores_\n",
        "top_filter_indices = np.argsort(filter_scores)[-60:]\n",
        "\n",
        "# Wrapper Method - RFE (use a valid estimator like KNN without invalid params)\n",
        "rfe_selector = RFE(estimator=RandomForestClassifier(n_estimators=100, random_state=42), n_features_to_select=30)\n",
        "rfe_selector.fit(X.iloc[:, top_filter_indices], y)\n",
        "rfe_selected_indices = np.array(top_filter_indices)[rfe_selector.support_]\n",
        "\n",
        "# Combine: RFE + random features to make 46\n",
        "remaining_indices = list(set(range(X.shape[1])) - set(rfe_selected_indices))\n",
        "extra_indices = np.random.choice(remaining_indices, size=16, replace=False)\n",
        "final_indices = list(rfe_selected_indices) + list(extra_indices)\n",
        "\n",
        "# Final selected features\n",
        "X_selected = X.iloc[:, final_indices[:46]]\n",
        "print(\"Selected Features Count:\", X_selected.shape[1])\n",
        "print(\"Selected Features:\", X_selected.columns.tolist())\n",
        "\n",
        "# Step 6: Split dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_selected, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Step 7: Train K-Nearest Neighbors model\n",
        "knn_model = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_model.fit(X_train, y_train)\n",
        "\n",
        "# Step 8: Evaluate the model\n",
        "y_pred = knn_model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred, average='weighted', zero_division=0)\n",
        "recall = recall_score(y_test, y_pred, average='weighted', zero_division=0)\n",
        "f1 = f1_score(y_test, y_pred, average='weighted', zero_division=0)\n",
        "\n",
        "print(\"\\nEvaluation Metrics:\")\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1 Score:\", f1)\n",
        "\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred, zero_division=0))\n",
        "\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q5z1YzSyUlPv",
        "outputId": "dd5e23c0-fe95-48b3-d7b7-f366ecb85155"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of Classes: 12\n",
            "Initial Feature Count: 46\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/feature_selection/_univariate_selection.py:783: UserWarning: k=60 is greater than n_features=46. All the features will be returned.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Selected Features Count: 40\n",
            "Selected Features: ['Number', 'Weight', 'IAT', 'Rate', 'Srate', 'Covariance', 'Duration', 'flow_duration', 'Tot size', 'Max', 'Tot sum', 'urg_count', 'fin_count', 'AVG', 'Min', 'Header_Length', 'Magnitue', 'rst_count', 'syn_count', 'Protocol Type', 'Variance', 'TCP', 'ack_count', 'syn_flag_number', 'UDP', 'ack_flag_number', 'ICMP', 'rst_flag_number', 'psh_flag_number', 'fin_flag_number', 'Std', 'DNS', 'cwr_flag_number', 'Telnet', 'SSH', 'Drate', 'Radius', 'LLC', 'HTTPS', 'IRC']\n",
            "\n",
            "Evaluation Metrics:\n",
            "Accuracy: 0.9515367686119754\n",
            "Precision: 0.9515135224655324\n",
            "Recall: 0.9515367686119754\n",
            "F1 Score: 0.9512176390956015\n",
            "\n",
            "Classification Report:\n",
            "                         precision    recall  f1-score   support\n",
            "\n",
            "          BenignTraffic       1.00      1.00      1.00      1748\n",
            "        DDoS-ICMP_Flood       1.00      1.00      1.00     11003\n",
            "      DDoS-PSHACK_Flood       1.00      1.00      1.00      6237\n",
            "       DDoS-RSTFINFlood       1.00      1.00      1.00      6318\n",
            "         DDoS-SYN_Flood       0.90      0.96      0.93      6195\n",
            "DDoS-SynonymousIP_Flood       0.97      0.94      0.95      5453\n",
            "         DDoS-TCP_Flood       0.94      0.96      0.95      6875\n",
            "         DDoS-UDP_Flood       0.91      0.94      0.92      8430\n",
            "          DoS-SYN_Flood       0.89      0.81      0.85      3047\n",
            "          DoS-TCP_Flood       0.94      0.89      0.91      4012\n",
            "          DoS-UDP_Flood       0.89      0.84      0.87      5059\n",
            "     Mirai-greeth_flood       1.00      0.99      1.00      1508\n",
            "\n",
            "               accuracy                           0.95     65885\n",
            "              macro avg       0.95      0.94      0.95     65885\n",
            "           weighted avg       0.95      0.95      0.95     65885\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            "[[ 1746     0     1     0     0     0     0     0     0     0     1     0]\n",
            " [    0 10995     1     1     2     1     0     0     1     0     1     1]\n",
            " [    1     0  6232     0     0     0     0     1     1     1     1     0]\n",
            " [    0     0     0  6314     0     0     0     1     0     0     3     0]\n",
            " [    0     1     1     1  5933    79     1     0   177     1     0     1]\n",
            " [    0     2     1     0   181  5139     0     1   129     0     0     0]\n",
            " [    0     4     0     1     1     0  6630     1     0   237     1     0]\n",
            " [    0     4     4     0     3     1     2  7902     2     0   512     0]\n",
            " [    0     5     6     0   455    95     1     1  2481     2     0     1]\n",
            " [    1     1     4     0     1     0   442     1     1  3560     1     0]\n",
            " [    0     1     0     0     0     0     2   791     1     1  4262     1]\n",
            " [    4     4     0     0     0     0     0     0     1     0     1  1498]]\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_selection import SelectKBest, chi2, RFE\n",
        "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix, precision_score, recall_score, f1_score\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Step 1: Load dataset\n",
        "df = pd.read_csv('data.csv')\n",
        "\n",
        "# Step 2: Keep only the 12 most common classes\n",
        "top_12_classes = df['label'].value_counts().index[:12]\n",
        "df_12 = df[df['label'].isin(top_12_classes)]\n",
        "\n",
        "# Step 3: Separate features and labels\n",
        "X = df_12.drop(\"label\", axis=1, errors='ignore')\n",
        "y = df_12[\"label\"]\n",
        "\n",
        "# Step 4: Preprocessing (encode categoricals + fill missing values + scale for chi2)\n",
        "X = pd.get_dummies(X, drop_first=True)\n",
        "X.fillna(X.mean(), inplace=True)\n",
        "X = pd.DataFrame(MinMaxScaler().fit_transform(X), columns=X.columns)  # chi2 requires non-negative values\n",
        "\n",
        "print(\"Number of Classes:\", y.nunique())\n",
        "print(\"Initial Feature Count:\", X.shape[1])\n",
        "\n",
        "# Step 5: Hybrid Feature Selection to get 46 features\n",
        "# Filter Method - Chi-Square\n",
        "filter_selector = SelectKBest(score_func=chi2, k=60)  # First filter to top 60 features\n",
        "X_filtered = filter_selector.fit_transform(X, y)\n",
        "filter_scores = filter_selector.scores_\n",
        "top_filter_indices = np.argsort(filter_scores)[-60:]\n",
        "\n",
        "# Wrapper Method - RFE (use a valid estimator like KNN without invalid params)\n",
        "rfe_selector = RFE(estimator=RandomForestClassifier(n_estimators=100, random_state=42), n_features_to_select=30)\n",
        "rfe_selector.fit(X.iloc[:, top_filter_indices], y)\n",
        "rfe_selected_indices = np.array(top_filter_indices)[rfe_selector.support_]\n",
        "\n",
        "# Combine: RFE + random features to make 46\n",
        "remaining_indices = list(set(range(X.shape[1])) - set(rfe_selected_indices))\n",
        "extra_indices = np.random.choice(remaining_indices, size=16, replace=False)\n",
        "final_indices = list(rfe_selected_indices) + list(extra_indices)\n",
        "\n",
        "# Final selected features\n",
        "X_selected = X.iloc[:, final_indices[:40]]\n",
        "print(\"Selected Features Count:\", X_selected.shape[1])\n",
        "print(\"Selected Features:\", X_selected.columns.tolist())\n",
        "\n",
        "# Step 6: Split dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_selected, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Step 7: Train K-Nearest Neighbors model\n",
        "knn_model = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_model.fit(X_train, y_train)\n",
        "\n",
        "# Step 8: Evaluate the model\n",
        "y_pred = knn_model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred, average='weighted', zero_division=0)\n",
        "recall = recall_score(y_test, y_pred, average='weighted', zero_division=0)\n",
        "f1 = f1_score(y_test, y_pred, average='weighted', zero_division=0)\n",
        "\n",
        "print(\"\\nEvaluation Metrics:\")\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1 Score:\", f1)\n",
        "\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred, zero_division=0))\n",
        "\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9gaFe2fIYm9v",
        "outputId": "1beef5b4-0b01-41f8-8082-427d85d909d0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of Classes: 12\n",
            "Initial Feature Count: 46\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/feature_selection/_univariate_selection.py:783: UserWarning: k=60 is greater than n_features=46. All the features will be returned.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Selected Features Count: 30\n",
            "Selected Features: ['Number', 'Weight', 'IAT', 'Rate', 'Srate', 'Covariance', 'Duration', 'flow_duration', 'Tot size', 'Max', 'Tot sum', 'urg_count', 'fin_count', 'AVG', 'Min', 'Header_Length', 'Magnitue', 'rst_count', 'syn_count', 'Protocol Type', 'Variance', 'TCP', 'ack_count', 'syn_flag_number', 'UDP', 'ack_flag_number', 'ICMP', 'rst_flag_number', 'psh_flag_number', 'fin_flag_number']\n",
            "\n",
            "Evaluation Metrics:\n",
            "Accuracy: 0.9525233361159596\n",
            "Precision: 0.9525307390839108\n",
            "Recall: 0.9525233361159596\n",
            "F1 Score: 0.9522409868248994\n",
            "\n",
            "Classification Report:\n",
            "                         precision    recall  f1-score   support\n",
            "\n",
            "          BenignTraffic       1.00      1.00      1.00      1748\n",
            "        DDoS-ICMP_Flood       1.00      1.00      1.00     11003\n",
            "      DDoS-PSHACK_Flood       1.00      1.00      1.00      6237\n",
            "       DDoS-RSTFINFlood       1.00      1.00      1.00      6318\n",
            "         DDoS-SYN_Flood       0.90      0.96      0.93      6195\n",
            "DDoS-SynonymousIP_Flood       0.97      0.94      0.96      5453\n",
            "         DDoS-TCP_Flood       0.94      0.97      0.95      6875\n",
            "         DDoS-UDP_Flood       0.91      0.94      0.92      8430\n",
            "          DoS-SYN_Flood       0.89      0.82      0.85      3047\n",
            "          DoS-TCP_Flood       0.94      0.89      0.92      4012\n",
            "          DoS-UDP_Flood       0.89      0.84      0.86      5059\n",
            "     Mirai-greeth_flood       1.00      0.99      1.00      1508\n",
            "\n",
            "               accuracy                           0.95     65885\n",
            "              macro avg       0.95      0.95      0.95     65885\n",
            "           weighted avg       0.95      0.95      0.95     65885\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            "[[ 1746     0     0     0     0     0     0     0     0     1     1     0]\n",
            " [    0 10994     1     0     2     0     0     0     1     1     4     0]\n",
            " [    0     0  6233     1     0     0     0     0     1     1     1     0]\n",
            " [    0     0     0  6315     0     0     0     0     1     0     2     0]\n",
            " [    0     0     1     1  5928    78     1     1   183     2     0     0]\n",
            " [    0     2     1     0   178  5145     0     1   126     0     0     0]\n",
            " [    0     3     0     1     0     0  6659     1     1   209     1     0]\n",
            " [    0     4     2     0     3     1     2  7897     3     0   518     0]\n",
            " [    0     4     5     0   439    83     2     0  2511     1     1     1]\n",
            " [    0     2     4     1     0     0   432     1     0  3570     2     0]\n",
            " [    0     1     0     0     1     0     2   791     1     1  4261     1]\n",
            " [    1     4     0     0     1     0     0     0     0     0     4  1498]]\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_selection import SelectKBest, chi2, RFE\n",
        "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix, precision_score, recall_score, f1_score\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Step 1: Load dataset\n",
        "df = pd.read_csv('data.csv')\n",
        "\n",
        "# Step 2: Keep only the 12 most common classes\n",
        "top_12_classes = df['label'].value_counts().index[:12]\n",
        "df_12 = df[df['label'].isin(top_12_classes)]\n",
        "\n",
        "# Step 3: Separate features and labels\n",
        "X = df_12.drop(\"label\", axis=1, errors='ignore')\n",
        "y = df_12[\"label\"]\n",
        "\n",
        "# Step 4: Preprocessing (encode categoricals + fill missing values + scale for chi2)\n",
        "X = pd.get_dummies(X, drop_first=True)\n",
        "X.fillna(X.mean(), inplace=True)\n",
        "X = pd.DataFrame(MinMaxScaler().fit_transform(X), columns=X.columns)  # chi2 requires non-negative values\n",
        "\n",
        "print(\"Number of Classes:\", y.nunique())\n",
        "print(\"Initial Feature Count:\", X.shape[1])\n",
        "\n",
        "# Step 5: Hybrid Feature Selection to get 46 features\n",
        "# Filter Method - Chi-Square\n",
        "filter_selector = SelectKBest(score_func=chi2, k=60)  # First filter to top 60 features\n",
        "X_filtered = filter_selector.fit_transform(X, y)\n",
        "filter_scores = filter_selector.scores_\n",
        "top_filter_indices = np.argsort(filter_scores)[-60:]\n",
        "\n",
        "# Wrapper Method - RFE (use a valid estimator like KNN without invalid params)\n",
        "rfe_selector = RFE(estimator=RandomForestClassifier(n_estimators=100, random_state=42), n_features_to_select=30)\n",
        "rfe_selector.fit(X.iloc[:, top_filter_indices], y)\n",
        "rfe_selected_indices = np.array(top_filter_indices)[rfe_selector.support_]\n",
        "\n",
        "# Combine: RFE + random features to make 46\n",
        "remaining_indices = list(set(range(X.shape[1])) - set(rfe_selected_indices))\n",
        "extra_indices = np.random.choice(remaining_indices, size=16, replace=False)\n",
        "final_indices = list(rfe_selected_indices) + list(extra_indices)\n",
        "\n",
        "# Final selected features\n",
        "X_selected = X.iloc[:, final_indices[:30]]\n",
        "print(\"Selected Features Count:\", X_selected.shape[1])\n",
        "print(\"Selected Features:\", X_selected.columns.tolist())\n",
        "\n",
        "# Step 6: Split dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_selected, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Step 7: Train K-Nearest Neighbors model\n",
        "knn_model = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_model.fit(X_train, y_train)\n",
        "\n",
        "# Step 8: Evaluate the model\n",
        "y_pred = knn_model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred, average='weighted', zero_division=0)\n",
        "recall = recall_score(y_test, y_pred, average='weighted', zero_division=0)\n",
        "f1 = f1_score(y_test, y_pred, average='weighted', zero_division=0)\n",
        "\n",
        "print(\"\\nEvaluation Metrics:\")\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1 Score:\", f1)\n",
        "\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred, zero_division=0))\n",
        "\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KOQsgv3YbnXM",
        "outputId": "afedbf94-b987-4dcb-dcb0-e5a16435820b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of Classes: 12\n",
            "Initial Feature Count: 46\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/feature_selection/_univariate_selection.py:783: UserWarning: k=60 is greater than n_features=46. All the features will be returned.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Selected Features Count: 20\n",
            "Selected Features: ['Number', 'Weight', 'IAT', 'Rate', 'Srate', 'Covariance', 'Duration', 'flow_duration', 'Tot size', 'Max', 'Tot sum', 'urg_count', 'fin_count', 'AVG', 'Min', 'Header_Length', 'Magnitue', 'rst_count', 'syn_count', 'Protocol Type']\n",
            "\n",
            "Evaluation Metrics:\n",
            "Accuracy: 0.9732260757380283\n",
            "Precision: 0.9731636998517462\n",
            "Recall: 0.9732260757380283\n",
            "F1 Score: 0.9731294536243194\n",
            "\n",
            "Classification Report:\n",
            "                         precision    recall  f1-score   support\n",
            "\n",
            "          BenignTraffic       1.00      1.00      1.00      1748\n",
            "        DDoS-ICMP_Flood       1.00      1.00      1.00     11003\n",
            "      DDoS-PSHACK_Flood       0.99      0.99      0.99      6237\n",
            "       DDoS-RSTFINFlood       1.00      1.00      1.00      6318\n",
            "         DDoS-SYN_Flood       0.96      0.97      0.96      6195\n",
            "DDoS-SynonymousIP_Flood       0.98      0.99      0.98      5453\n",
            "         DDoS-TCP_Flood       0.97      0.98      0.98      6875\n",
            "         DDoS-UDP_Flood       0.94      0.96      0.95      8430\n",
            "          DoS-SYN_Flood       0.95      0.91      0.93      3047\n",
            "          DoS-TCP_Flood       0.97      0.95      0.96      4012\n",
            "          DoS-UDP_Flood       0.93      0.90      0.92      5059\n",
            "     Mirai-greeth_flood       1.00      0.99      1.00      1508\n",
            "\n",
            "               accuracy                           0.97     65885\n",
            "              macro avg       0.97      0.97      0.97     65885\n",
            "           weighted avg       0.97      0.97      0.97     65885\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            "[[ 1747     0     0     0     0     0     0     0     0     1     0     0]\n",
            " [    2 10993     0     1     1     0     1     0     1     1     3     0]\n",
            " [    2     0  6176     6     0     0    36     0     1    14     1     1]\n",
            " [    1     0     2  6309     1     0     0     0     2     2     1     0]\n",
            " [    0     0     0     1  6004    60     0     2   125     1     1     1]\n",
            " [    1     0     1     0    52  5379     0     0    17     1     2     0]\n",
            " [    1     0    25     0     1     0  6762     0     2    80     3     1]\n",
            " [    0     2     2     1     2     1     0  8107     4     1   309     1]\n",
            " [    0     2     0     0   207    61     0     3  2768     2     3     1]\n",
            " [    0     1    32    11     0     0   155     0     1  3809     3     0]\n",
            " [    1     2     0     0     1     0     0   484     2     1  4568     0]\n",
            " [    0     1     1     1     0     0     0     0     1     0     5  1499]]\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_selection import SelectKBest, chi2, RFE\n",
        "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix, precision_score, recall_score, f1_score\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Step 1: Load dataset\n",
        "df = pd.read_csv('data.csv')\n",
        "\n",
        "# Step 2: Keep only the 12 most common classes\n",
        "top_12_classes = df['label'].value_counts().index[:12]\n",
        "df_12 = df[df['label'].isin(top_12_classes)]\n",
        "\n",
        "# Step 3: Separate features and labels\n",
        "X = df_12.drop(\"label\", axis=1, errors='ignore')\n",
        "y = df_12[\"label\"]\n",
        "\n",
        "# Step 4: Preprocessing (encode categoricals + fill missing values + scale for chi2)\n",
        "X = pd.get_dummies(X, drop_first=True)\n",
        "X.fillna(X.mean(), inplace=True)\n",
        "X = pd.DataFrame(MinMaxScaler().fit_transform(X), columns=X.columns)  # chi2 requires non-negative values\n",
        "\n",
        "print(\"Number of Classes:\", y.nunique())\n",
        "print(\"Initial Feature Count:\", X.shape[1])\n",
        "\n",
        "# Step 5: Hybrid Feature Selection to get 46 features\n",
        "# Filter Method - Chi-Square\n",
        "filter_selector = SelectKBest(score_func=chi2, k=60)  # First filter to top 60 features\n",
        "X_filtered = filter_selector.fit_transform(X, y)\n",
        "filter_scores = filter_selector.scores_\n",
        "top_filter_indices = np.argsort(filter_scores)[-60:]\n",
        "\n",
        "# Wrapper Method - RFE (use a valid estimator like KNN without invalid params)\n",
        "rfe_selector = RFE(estimator=RandomForestClassifier(n_estimators=100, random_state=42), n_features_to_select=30)\n",
        "rfe_selector.fit(X.iloc[:, top_filter_indices], y)\n",
        "rfe_selected_indices = np.array(top_filter_indices)[rfe_selector.support_]\n",
        "\n",
        "# Combine: RFE + random features to make 46\n",
        "remaining_indices = list(set(range(X.shape[1])) - set(rfe_selected_indices))\n",
        "extra_indices = np.random.choice(remaining_indices, size=16, replace=False)\n",
        "final_indices = list(rfe_selected_indices) + list(extra_indices)\n",
        "\n",
        "# Final selected features\n",
        "X_selected = X.iloc[:, final_indices[:20]]\n",
        "print(\"Selected Features Count:\", X_selected.shape[1])\n",
        "print(\"Selected Features:\", X_selected.columns.tolist())\n",
        "\n",
        "# Step 6: Split dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_selected, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Step 7: Train K-Nearest Neighbors model\n",
        "knn_model = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_model.fit(X_train, y_train)\n",
        "\n",
        "# Step 8: Evaluate the model\n",
        "y_pred = knn_model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred, average='weighted', zero_division=0)\n",
        "recall = recall_score(y_test, y_pred, average='weighted', zero_division=0)\n",
        "f1 = f1_score(y_test, y_pred, average='weighted', zero_division=0)\n",
        "\n",
        "print(\"\\nEvaluation Metrics:\")\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1 Score:\", f1)\n",
        "\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred, zero_division=0))\n",
        "\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fZsOqNMGeqeL",
        "outputId": "bf7fa5fc-43dc-4abe-8e00-2cc9a1c9053a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of Classes: 12\n",
            "Initial Feature Count: 46\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/feature_selection/_univariate_selection.py:783: UserWarning: k=60 is greater than n_features=46. All the features will be returned.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Selected Features Count: 15\n",
            "Selected Features: ['Number', 'Weight', 'IAT', 'Rate', 'Srate', 'Covariance', 'Duration', 'flow_duration', 'Tot size', 'Max', 'Tot sum', 'urg_count', 'fin_count', 'AVG', 'Min']\n",
            "\n",
            "Evaluation Metrics:\n",
            "Accuracy: 0.9750018972451999\n",
            "Precision: 0.9749685974128957\n",
            "Recall: 0.9750018972451999\n",
            "F1 Score: 0.9749773604410966\n",
            "\n",
            "Classification Report:\n",
            "                         precision    recall  f1-score   support\n",
            "\n",
            "          BenignTraffic       1.00      1.00      1.00      1748\n",
            "        DDoS-ICMP_Flood       0.99      0.99      0.99     11003\n",
            "      DDoS-PSHACK_Flood       0.98      0.99      0.98      6237\n",
            "       DDoS-RSTFINFlood       1.00      1.00      1.00      6318\n",
            "         DDoS-SYN_Flood       0.96      0.95      0.95      6195\n",
            "DDoS-SynonymousIP_Flood       0.99      0.99      0.99      5453\n",
            "         DDoS-TCP_Flood       0.96      0.97      0.96      6875\n",
            "         DDoS-UDP_Flood       0.98      0.98      0.98      8430\n",
            "          DoS-SYN_Flood       0.92      0.92      0.92      3047\n",
            "          DoS-TCP_Flood       0.95      0.94      0.95      4012\n",
            "          DoS-UDP_Flood       0.96      0.96      0.96      5059\n",
            "     Mirai-greeth_flood       0.99      0.99      0.99      1508\n",
            "\n",
            "               accuracy                           0.98     65885\n",
            "              macro avg       0.97      0.97      0.97     65885\n",
            "           weighted avg       0.97      0.98      0.97     65885\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            "[[ 1746     0     0     0     0     0     0     0     0     1     1     0]\n",
            " [    2 10941     4     0     7     4     3    22     2     5    11     2]\n",
            " [    0     7  6145     6     9    36     2     9    13     4     5     1]\n",
            " [    1     0     0  6305     1     1     0     1     6     0     1     2]\n",
            " [    0     1    24     0  5893     7   197     5    45     5    18     0]\n",
            " [    1     8    40     0     8  5383     0     6     1     2     4     0]\n",
            " [    0     7     5     0   127     4  6671     6    19    13    22     1]\n",
            " [    0     9    14     0    13     1     4  8239     5    10   134     1]\n",
            " [    0     5     9    13    39     0    33     8  2794   137     8     1]\n",
            " [    0    10     4     3    34     1    32     6   129  3777    14     2]\n",
            " [    0    29     5     0    19     0     9   134     7     7  4848     1]\n",
            " [    1     1     2     2     0     0     0     0     0     1     5  1496]]\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_selection import SelectKBest, chi2, RFE\n",
        "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix, precision_score, recall_score, f1_score\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Step 1: Load dataset\n",
        "df = pd.read_csv('data.csv')\n",
        "\n",
        "# Step 2: Keep only the 12 most common classes\n",
        "top_12_classes = df['label'].value_counts().index[:12]\n",
        "df_12 = df[df['label'].isin(top_12_classes)]\n",
        "\n",
        "# Step 3: Separate features and labels\n",
        "X = df_12.drop(\"label\", axis=1, errors='ignore')\n",
        "y = df_12[\"label\"]\n",
        "\n",
        "# Step 4: Preprocessing (encode categoricals + fill missing values + scale for chi2)\n",
        "X = pd.get_dummies(X, drop_first=True)\n",
        "X.fillna(X.mean(), inplace=True)\n",
        "X = pd.DataFrame(MinMaxScaler().fit_transform(X), columns=X.columns)  # chi2 requires non-negative values\n",
        "\n",
        "print(\"Number of Classes:\", y.nunique())\n",
        "print(\"Initial Feature Count:\", X.shape[1])\n",
        "\n",
        "# Step 5: Hybrid Feature Selection to get 46 features\n",
        "# Filter Method - Chi-Square\n",
        "filter_selector = SelectKBest(score_func=chi2, k=60)  # First filter to top 60 features\n",
        "X_filtered = filter_selector.fit_transform(X, y)\n",
        "filter_scores = filter_selector.scores_\n",
        "top_filter_indices = np.argsort(filter_scores)[-60:]\n",
        "\n",
        "# Wrapper Method - RFE (use a valid estimator like KNN without invalid params)\n",
        "rfe_selector = RFE(estimator=RandomForestClassifier(n_estimators=100, random_state=42), n_features_to_select=30)\n",
        "rfe_selector.fit(X.iloc[:, top_filter_indices], y)\n",
        "rfe_selected_indices = np.array(top_filter_indices)[rfe_selector.support_]\n",
        "\n",
        "# Combine: RFE + random features to make 46\n",
        "remaining_indices = list(set(range(X.shape[1])) - set(rfe_selected_indices))\n",
        "extra_indices = np.random.choice(remaining_indices, size=16, replace=False)\n",
        "final_indices = list(rfe_selected_indices) + list(extra_indices)\n",
        "\n",
        "# Final selected features\n",
        "X_selected = X.iloc[:, final_indices[:15]]\n",
        "print(\"Selected Features Count:\", X_selected.shape[1])\n",
        "print(\"Selected Features:\", X_selected.columns.tolist())\n",
        "\n",
        "# Step 6: Split dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_selected, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Step 7: Train K-Nearest Neighbors model\n",
        "knn_model = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_model.fit(X_train, y_train)\n",
        "\n",
        "# Step 8: Evaluate the model\n",
        "y_pred = knn_model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred, average='weighted', zero_division=0)\n",
        "recall = recall_score(y_test, y_pred, average='weighted', zero_division=0)\n",
        "f1 = f1_score(y_test, y_pred, average='weighted', zero_division=0)\n",
        "\n",
        "print(\"\\nEvaluation Metrics:\")\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1 Score:\", f1)\n",
        "\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred, zero_division=0))\n",
        "\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "HOcwzpghhdKY",
        "outputId": "3585d08a-6834-48fc-ef3e-28d746cf0b6f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of Classes: 12\n",
            "Initial Feature Count: 46\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/feature_selection/_univariate_selection.py:783: UserWarning: k=60 is greater than n_features=46. All the features will be returned.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Selected Features Count: 10\n",
            "Selected Features: ['Number', 'Weight', 'IAT', 'Rate', 'Srate', 'Covariance', 'Duration', 'flow_duration', 'Tot size', 'Max']\n",
            "\n",
            "Evaluation Metrics:\n",
            "Accuracy: 0.9717689914244517\n",
            "Precision: 0.9718026642703845\n",
            "Recall: 0.9717689914244517\n",
            "F1 Score: 0.9717735817330868\n",
            "\n",
            "Classification Report:\n",
            "                         precision    recall  f1-score   support\n",
            "\n",
            "          BenignTraffic       1.00      1.00      1.00      1748\n",
            "        DDoS-ICMP_Flood       0.99      0.99      0.99     11003\n",
            "      DDoS-PSHACK_Flood       0.97      0.97      0.97      6237\n",
            "       DDoS-RSTFINFlood       0.97      0.97      0.97      6318\n",
            "         DDoS-SYN_Flood       0.96      0.96      0.96      6195\n",
            "DDoS-SynonymousIP_Flood       0.99      0.98      0.98      5453\n",
            "         DDoS-TCP_Flood       0.97      0.97      0.97      6875\n",
            "         DDoS-UDP_Flood       0.97      0.98      0.97      8430\n",
            "          DoS-SYN_Flood       0.93      0.94      0.94      3047\n",
            "          DoS-TCP_Flood       0.96      0.95      0.96      4012\n",
            "          DoS-UDP_Flood       0.95      0.96      0.95      5059\n",
            "     Mirai-greeth_flood       0.99      0.99      0.99      1508\n",
            "\n",
            "               accuracy                           0.97     65885\n",
            "              macro avg       0.97      0.97      0.97     65885\n",
            "           weighted avg       0.97      0.97      0.97     65885\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            "[[ 1747     0     0     0     0     0     0     0     0     0     0     1]\n",
            " [    2 10889     4    10    24     1     4    48     4     4    12     1]\n",
            " [    2    12  6079    87     4    30     2    11     1     3     5     1]\n",
            " [    1    10    97  6144     3    32     2    15     2     3     9     0]\n",
            " [    0     9     5     5  5940     6   170    18    12     7    23     0]\n",
            " [    1     8    35    75     2  5318     1     7     1     2     3     0]\n",
            " [    1    17     2     1   116     0  6678    13    19     4    22     2]\n",
            " [    0    20     6     6    32     1    11  8226     6     5   116     1]\n",
            " [    0     4     4     2    19     0    11    12  2865   102    26     2]\n",
            " [    0    13     2     5    12     0    11     5   129  3800    35     0]\n",
            " [    0    13     6     2    12     0    20   126    26     8  4845     1]\n",
            " [    1     1     2     1     0     1     1     0     0     1     6  1494]]\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_selection import SelectKBest, chi2, RFE\n",
        "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix, precision_score, recall_score, f1_score\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Step 1: Load dataset\n",
        "df = pd.read_csv('data.csv')\n",
        "\n",
        "# Step 2: Keep only the 12 most common classes\n",
        "top_12_classes = df['label'].value_counts().index[:12]\n",
        "df_12 = df[df['label'].isin(top_12_classes)]\n",
        "\n",
        "# Step 3: Separate features and labels\n",
        "X = df_12.drop(\"label\", axis=1, errors='ignore')\n",
        "y = df_12[\"label\"]\n",
        "\n",
        "# Step 4: Preprocessing (encode categoricals + fill missing values + scale for chi2)\n",
        "X = pd.get_dummies(X, drop_first=True)\n",
        "X.fillna(X.mean(), inplace=True)\n",
        "X = pd.DataFrame(MinMaxScaler().fit_transform(X), columns=X.columns)  # chi2 requires non-negative values\n",
        "\n",
        "print(\"Number of Classes:\", y.nunique())\n",
        "print(\"Initial Feature Count:\", X.shape[1])\n",
        "\n",
        "# Step 5: Hybrid Feature Selection to get 46 features\n",
        "# Filter Method - Chi-Square\n",
        "filter_selector = SelectKBest(score_func=chi2, k=60)  # First filter to top 60 features\n",
        "X_filtered = filter_selector.fit_transform(X, y)\n",
        "filter_scores = filter_selector.scores_\n",
        "top_filter_indices = np.argsort(filter_scores)[-60:]\n",
        "\n",
        "# Wrapper Method - RFE (use a valid estimator like KNN without invalid params)\n",
        "rfe_selector = RFE(estimator=RandomForestClassifier(n_estimators=100, random_state=42), n_features_to_select=30)\n",
        "rfe_selector.fit(X.iloc[:, top_filter_indices], y)\n",
        "rfe_selected_indices = np.array(top_filter_indices)[rfe_selector.support_]\n",
        "\n",
        "# Combine: RFE + random features to make 46\n",
        "remaining_indices = list(set(range(X.shape[1])) - set(rfe_selected_indices))\n",
        "extra_indices = np.random.choice(remaining_indices, size=16, replace=False)\n",
        "final_indices = list(rfe_selected_indices) + list(extra_indices)\n",
        "\n",
        "# Final selected features\n",
        "X_selected = X.iloc[:, final_indices[:10]]\n",
        "print(\"Selected Features Count:\", X_selected.shape[1])\n",
        "print(\"Selected Features:\", X_selected.columns.tolist())\n",
        "\n",
        "# Step 6: Split dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_selected, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Step 7: Train K-Nearest Neighbors model\n",
        "knn_model = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_model.fit(X_train, y_train)\n",
        "\n",
        "# Step 8: Evaluate the model\n",
        "y_pred = knn_model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred, average='weighted', zero_division=0)\n",
        "recall = recall_score(y_test, y_pred, average='weighted', zero_division=0)\n",
        "f1 = f1_score(y_test, y_pred, average='weighted', zero_division=0)\n",
        "\n",
        "print(\"\\nEvaluation Metrics:\")\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1 Score:\", f1)\n",
        "\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred, zero_division=0))\n",
        "\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "pK8Bpi_ckhRs",
        "outputId": "571ab95e-f398-42d2-931a-121cc1231f41"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of Classes: 12\n",
            "Initial Feature Count: 46\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/feature_selection/_univariate_selection.py:783: UserWarning: k=60 is greater than n_features=46. All the features will be returned.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Selected Features Count: 5\n",
            "Selected Features: ['Number', 'Weight', 'IAT', 'Rate', 'Srate']\n",
            "\n",
            "Evaluation Metrics:\n",
            "Accuracy: 0.9926418786692759\n",
            "Precision: 0.9926783448896348\n",
            "Recall: 0.9926418786692759\n",
            "F1 Score: 0.9926506749548951\n",
            "\n",
            "Classification Report:\n",
            "                         precision    recall  f1-score   support\n",
            "\n",
            "          BenignTraffic       1.00      1.00      1.00       984\n",
            "        DDoS-ICMP_Flood       0.99      0.99      0.99      6430\n",
            "      DDoS-PSHACK_Flood       1.00      0.99      0.99      3672\n",
            "       DDoS-RSTFINFlood       0.99      1.00      0.99      3618\n",
            "         DDoS-SYN_Flood       1.00      0.99      0.99      3613\n",
            "DDoS-SynonymousIP_Flood       1.00      0.99      1.00      3192\n",
            "         DDoS-TCP_Flood       1.00      0.99      0.99      4024\n",
            "         DDoS-UDP_Flood       0.98      0.99      0.98      4833\n",
            "          DoS-SYN_Flood       1.00      0.99      1.00      1763\n",
            "          DoS-TCP_Flood       1.00      1.00      1.00      2354\n",
            "          DoS-UDP_Flood       0.99      0.99      0.99      2957\n",
            "     Mirai-greeth_flood       1.00      0.99      0.99       885\n",
            "\n",
            "               accuracy                           0.99     38325\n",
            "              macro avg       0.99      0.99      0.99     38325\n",
            "           weighted avg       0.99      0.99      0.99     38325\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            "[[ 983    1    0    0    0    0    0    0    0    0    0    0]\n",
            " [   1 6369    3    1    0    3    1   49    0    0    3    0]\n",
            " [   0    5 3644   14    1    0    2    4    0    0    2    0]\n",
            " [   0    5    3 3603    0    1    0    5    0    0    0    1]\n",
            " [   0    4    0    0 3576    0   11   22    0    0    0    0]\n",
            " [   1    6    2    3    0 3176    0    3    0    0    1    0]\n",
            " [   0    1    0    0    6    0 3999    8    0    0   10    0]\n",
            " [   0   24    2    3    1    0    0 4793    0    0   10    0]\n",
            " [   0    3    0    0    0    0    0    0 1754    3    3    0]\n",
            " [   1    0    0    0    0    0    1    0    2 2343    7    0]\n",
            " [   1    7    1    1    0    0    4   13    1    0 2929    0]\n",
            " [   0    3    0    3    0    0    1    2    0    2    0  874]]\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_selection import SelectKBest, chi2, RFE\n",
        "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix, precision_score, recall_score, f1_score\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Step 1: Load dataset\n",
        "df = pd.read_csv('data.csv')\n",
        "\n",
        "# Step 2: Keep only the 12 most common classes\n",
        "top_12_classes = df['label'].value_counts().index[:12]\n",
        "df_12 = df[df['label'].isin(top_12_classes)]\n",
        "\n",
        "# Step 3: Separate features and labels\n",
        "X = df_12.drop(\"label\", axis=1, errors='ignore')\n",
        "y = df_12[\"label\"]\n",
        "\n",
        "# Step 4: Preprocessing (encode categoricals + fill missing values + scale for chi2)\n",
        "X = pd.get_dummies(X, drop_first=True)\n",
        "X.fillna(X.mean(), inplace=True)\n",
        "X = pd.DataFrame(MinMaxScaler().fit_transform(X), columns=X.columns)  # chi2 requires non-negative values\n",
        "\n",
        "print(\"Number of Classes:\", y.nunique())\n",
        "print(\"Initial Feature Count:\", X.shape[1])\n",
        "\n",
        "# Step 5: Hybrid Feature Selection to get 46 features\n",
        "# Filter Method - Chi-Square\n",
        "filter_selector = SelectKBest(score_func=chi2, k=60)  # First filter to top 60 features\n",
        "X_filtered = filter_selector.fit_transform(X, y)\n",
        "filter_scores = filter_selector.scores_\n",
        "top_filter_indices = np.argsort(filter_scores)[-60:]\n",
        "\n",
        "# Wrapper Method - RFE (use a valid estimator like KNN without invalid params)\n",
        "rfe_selector = RFE(estimator=RandomForestClassifier(n_estimators=100, random_state=42), n_features_to_select=30)\n",
        "rfe_selector.fit(X.iloc[:, top_filter_indices], y)\n",
        "rfe_selected_indices = np.array(top_filter_indices)[rfe_selector.support_]\n",
        "\n",
        "# Combine: RFE + random features to make 46\n",
        "remaining_indices = list(set(range(X.shape[1])) - set(rfe_selected_indices))\n",
        "extra_indices = np.random.choice(remaining_indices, size=16, replace=False)\n",
        "final_indices = list(rfe_selected_indices) + list(extra_indices)\n",
        "\n",
        "# Final selected features\n",
        "X_selected = X.iloc[:, final_indices[:5]]\n",
        "print(\"Selected Features Count:\", X_selected.shape[1])\n",
        "print(\"Selected Features:\", X_selected.columns.tolist())\n",
        "\n",
        "# Step 6: Split dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_selected, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Step 7: Train K-Nearest Neighbors model\n",
        "knn_model = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_model.fit(X_train, y_train)\n",
        "\n",
        "# Step 8: Evaluate the model\n",
        "y_pred = knn_model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred, average='weighted', zero_division=0)\n",
        "recall = recall_score(y_test, y_pred, average='weighted', zero_division=0)\n",
        "f1 = f1_score(y_test, y_pred, average='weighted', zero_division=0)\n",
        "\n",
        "print(\"\\nEvaluation Metrics:\")\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1 Score:\", f1)\n",
        "\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred, zero_division=0))\n",
        "\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred))\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}